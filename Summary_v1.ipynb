{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxCgrsy5CrrFXgcLezMy6t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xlumzee/FinePrint/blob/main/Summary_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "S7ndScXgGLWr"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the required stopwords and tokenizers\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5kOIrtrGe2m",
        "outputId": "62b7c211-5ecc-49d5-b629-118f4a88bc3b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a sample text to summarize\n",
        "\n",
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "The fox is running fast to catch its prey.\n",
        "Suddenly, it sees the prey and jumps over it. The prey is escapes and the fox continous to run.\n",
        "The dog wakes up and barks at the fox.\n",
        "The fox runs away and the dog does back to sleep.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oUvlvRshGpoq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "UZeuq4peHGu8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords and stem the words in each sentence\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Build a frequency distribution over stemmed, non stopword, alphabetic tokens\n",
        "tokens = []\n",
        "for sentence in sentences:\n",
        "  for w in word_tokenize(sentence):\n",
        "    if w not in stop_words and w.isalpha():\n",
        "      tokens.append(stemmer.stem(w))"
      ],
      "metadata": {
        "id": "U_9MnlbZHQOr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the count of frequency of each word in the text\n",
        "\n",
        "freq_dist = nltk.FreqDist(tokens)"
      ],
      "metadata": {
        "id": "6P6eeDtDIhhG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 10\n",
        "top_stems = {w for w, _ in freq_dist.most_common(top_k)}"
      ],
      "metadata": {
        "id": "kvexwU8rVgKJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Score each sentence by summing frequencies of its (filtered, stemmed) words\n",
        "scored = []\n",
        "for sentence in sentences:\n",
        "  sent_tokens = [stemmer.stem(w) for w in word_tokenize(sentence.lower()) if w.isalpha() and w not in stop_words]\n",
        "  score = sum(freq_dist[s] for s in sent_tokens if s in top_stems)\n",
        "  scored.append((sentence, score))\n"
      ],
      "metadata": {
        "id": "5KgET67qUe2B"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 3\n",
        "for sentence, _ in nlargest(N, scored, key=lambda x: x[1]):\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNfJDHQ2K9xb",
        "outputId": "46ec85f2-4d6f-4a0b-8be9-7a3a74a899d4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The quick brown fox jumps over the lazy dog.\n",
            "The fox is running fast to catch its prey.\n",
            "The prey is escapes and the fox continous to run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KsAvx-uiV6Be"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}