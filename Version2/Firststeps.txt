Order of Implementation
1) 
Raw Policy → Extract → Clean → Summarize → Output
made an Extractor.py, summarizer.py using an llm from openai - giving it a prompt and api call. loads a policy and summarizes in 5 bullet points
Done 

2) 
Handle long policies - Chunking + combining summaries. Big companies have big policies - far too long for a single llm call.
Chunk long text → summarize each chunk → combine summaries → produce final summary
Scaling
Done

3)
Risk detection engine
Detect important clause categories
Highlight the risky or user - impactful clauses
- First rule based detection
Done

4) 
ML Classifier taht learns to detect clauses automatically
Create a Dataset(CSV) with clause_text and label using the rule based detector. This should ideally cature discovered clause, detected label, context and this will become training data

    4.1) 
    Clause Segmentation - split policy into analyzable clauses
    clause splitter

    4.2) 
    auto label each clause using rule based detector (Next to do)
    auto_labeler will import the clause splitter, import risk detector, label each clause and return a list of samples

    4.3) building the clause dataset
    columns be like clause, label, source_policy
    
--- Gonna make a better rule based risk pattern before bert for a better data set
done

also collected data on google, tiktok, delta
ran pipelines on the above


Progress and updates
- labels for the collected privacy policies 
none            742
tracking         47
data_sharing     44
location          4
ai_decisions      4
refunds           1

this is too low, i need atleast 20 samples on each label, so need to collect more data, will do that



5) Bert training - next to do 
