Order of Implementation
1) 
Raw Policy → Extract → Clean → Summarize → Output
made an Extractor.py, summarizer.py using an llm from openai - giving it a prompt and api call. loads a policy and summarizes in 5 bullet points
Done 

2) 
Handle long policies - Chunking + combining summaries. Big companies have big policies - far too long for a single llm call.
Chunk long text → summarize each chunk → combine summaries → produce final summary
Scaling
Done

3)
Risk detection engine
Detect important clause categories
Highlight the risky or user - impactful clauses
- First rule based detection
Done

4) 
ML Classifier taht learns to detect clauses automatically
Create a Dataset(CSV) with clause_text and label using the rule based detector. This should ideally cature discovered clause, detected label, context and this will become training data

    4.1) 
    Clause Segmentation - split policy into analyzable clauses
    clause splitter

    4.2) 
    auto label each clause using rule based detector (Next to do)
    auto_labeler will import the clause splitter, import risk detector, label each clause and return a list of samples

    4.3) building the clause dataset
    columns be like clause, label, source_policy
    
--- Gonna make a better rule based risk pattern before bert for a better data set
done

also collected data on google, tiktok, delta
ran pipelines on the above


Progress and updates
- labels for the collected privacy policies 
none                 3200
data_sharing          310
tracking              214
location               24
ai_decisions           11
employment_checks       5
arbitration             2
refunds                 1

this is too low, i need atleast 20 samples on each label, so need to collect more data, will do that

Collecting more data for AI decisions, location, employment checks and refunds
Collecting data is lowkey the most boring part. I am going to each website and copy pasting the privacy policy. Soooo boring. I wonder if there is a faster way. But well, this is part of the process. 
Whoever you are, If you are reading this in the future, reach out to me on linkedin, and tell me remember boston clock reset? I will for sure help you through whatever you are going through. I need to put this someplace more obscure.

COU - stands for Conditions of Use
TOU/TOS - Terms of use/ Terms of service - some websites have slow loading pages and very laggy
I need to make a glossary. and an acronym list

Some companies have a dedicated help center, while some have a single page. 

Trying to target 15 to 20 more policies before i start training bert. 

Get 15 to 20 policies 
run the pipeline

Going to create a universal pipeline script, i dont want to run everything individually.

new count 
label
none                 12030
data_sharing           843
tracking               562
refunds                136
location                92
arbitration             61
ai_decisions            56
employment_checks       16
auto_renewal             2
Name: count, dtype: int64

need to clean

cleaned version 

label
none                 11627
data_sharing           768
tracking               526
refunds                134
location                81
arbitration             59
ai_decisions            51
employment_checks       14
auto_renewal             2
Name: count, dtype: int64

collecting the data was worth it. 
Can train a strong 7 class transformer. Need to drop employment_checks and auto_renewal. They can cause mispredictions and hurt training.

Time to start step 5 , after gym tho. 

Intalling dependencies
pip install transformers datasets accelerate scikit-learn onnx onnxruntime

7 final classes as labels
	•	none
	•	data_sharing
	•	tracking
	•	refunds
	•	location
	•	arbitration
	•	ai_decisions


train test split done 



-- issues with transformer library 
rebuilding environment using pyenv for 3.12.1 -- currently on 3.13.7 which is giving errors
5) Bert training - next to do 
