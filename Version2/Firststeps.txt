Order of Implementation
1) 
Raw Policy → Extract → Clean → Summarize → Output
made an Extractor.py, summarizer.py using an llm from openai - giving it a prompt and api call. loads a policy and summarizes in 5 bullet points
Done 

2) 
Handle long policies - Chunking + combining summaries. Big companies have big policies - far too long for a single llm call.
Chunk long text → summarize each chunk → combine summaries → produce final summary
Scaling
Done

3)
Risk detection engine
Detect important clause categories
Highlight the risky or user - impactful clauses
- First rule based detection
Done

4) 
ML Classifier taht learns to detect clauses automatically
Create a Dataset(CSV) with clause_text and label using the rule based detector. This should ideally cature discovered clause, detected label, context and this will become training data

    4.1) 
    Clause Segmentation - split policy into analyzable clauses
    clause splitter

    4.2) 
    auto label each clause using rule based detector (Next to do)
    auto_labeler will import the clause splitter, import risk detector, label each clause and return a list of samples

    4.3) building the clause dataset